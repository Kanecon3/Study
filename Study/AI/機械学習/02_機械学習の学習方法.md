# 機械学習の学習方法

## 教師あり学習

正解データ（ラベル）を提供することでコンピューターに学習させる手法です。

たとえば、大量の動物の画像データに、「これは“ネコ”」「これは“イヌ”」・・・といったようにあらかじめラベリングをしておきます。十分なデータを用意し、それらを教師（正解）として学習を行います。

教師あり学習は、正解・不正解が明確な問題の解決に利用できます。

## 教師なし学習

TODO:

## 強化学習

TODO:

## 参考

GPT-4などの大規模言語モデル（LLM）は、どうやって学習データを作成しているのか調査してみました。

### データの収集

大規模言語モデルのトレーニングには、まず膨大な量のテキストデータを収集する必要があります。データ収集には主に以下の方法が用いられています。

#### ウェブデータの収集

LLMに必要なテキストデータの大部分はインターネット上から収集されます。一般的な収集方法としては以下のような方法があります。

- **Webスクレイピング**: ニュースサイトやWikipediaなどのウェブページから情報を抽出します
- **Common Crawl（コモンクロール）**: ペタバイト級のデータ量を持つオープンソースのウェブクローリングデータベースで、多くのLLMの学習データとして広く利用されています
- **APIによるデータ取得**: TwitterやRedditなどのウェブサービスがAPI経由で提供するデータを利用します

#### 書籍データの活用

質の高いテキストデータとして書籍も重要な情報源となっています。

- **BookCorpus（ブックコーパス）**: GPTモデルやBERTなどで初期的に使用されたデータセットで、幅広いジャンルをカバーする11,000以上の書籍から構成されています
- **電子図書館**: 小説、エッセイ、詩、ドラマ、歴史、科学、哲学などのパブリックドメイン作品を含む7万冊以上の文学書が利用されます

#### その他のデータソース

- **Reddit**: ユーザーがコンテンツを投稿し、評価システム（upvote/downvote）によって質が判断されるソーシャルメディアからのデータ
- **学術論文**: 専門的な知識を学習するための情報源
- **コードリポジトリ**: プログラミング能力を持たせるためのソースコード

OpenAIの公式情報によると、GPT-4の学習データには「公開データ（publicly available data）」と「ライセンスを取得したデータ（data we've licensed）」の両方が使用されています。

### データの前処理とクリーニング

収集した生データには、ノイズや不要な情報が含まれているため、高品質なデータセットを作成するためにはクリーニングが必須です。主な前処理手法には以下のものがあります。

#### 不要要素の除去

- **HTMLタグの除去**: Webスクレイピングで収集したデータには多くのHTMLタグが含まれているため、これらを取り除きます
- **特殊記号や余分な空白の削除**: テキストの構造を整理し、モデルが正確に学習できるようにします
- **広告やスパムのフィルタリング**: LLMに無関係な情報を除去します

#### データの品質向上

- **重複データの削除**: データの冗長性を減らすために、同じ内容が繰り返されているテキストを削除します。これにはMinHashなどのアルゴリズムが使用されます
- **言語判定**: 特定の言語のデータのみを抽出するために、言語判定ツールを使用します
- **文字数・文の構造によるフィルタリング**: 文章として成り立っていないデータ（例：「#朝食メニュー スクランブルエッグ クロワッサン コーヒー」）を除外します

#### 倫理とプライバシーへの配慮

- **有害コンテンツの除去**: 差別的表現や暴力的内容を含むテキストを排除します
- **個人情報のマスキング**: プライバシー保護のため、個人を特定できる情報をマスキングまたは削除します
- **バイアスの軽減**: 性別、人種、年齢などに関するバイアスを軽減するため、データのサンプリング方法を調整します

### データの変換とトークン化

前処理後、LLMがテキストデータを理解できる形式に変換する必要があります。

#### トークン化

- **単語トークン化**: テキストを単語ごとに分割する方法で、英語などのスペース区切りの言語に有効です
- **サブワードトークン化**: 単語をさらに細かく分割し、語の一部に分解する技術。BPE（Byte Pair Encoding）やWordPieceなどが使用されます
- **文字トークン化**: 文字単位でトークン化する手法で、どんな言語や文字体系にも対応可能です

#### テキストのベクトル化

- **TF-IDF**: 単語の出現頻度と重要度を数値化する手法
- **Word2Vec**: 単語を低次元の密ベクトルに変換する手法
- **BERT埋め込み**: 文脈を考慮した単語の表現を生成する手法

### LLMの学習プロセス

GPT-4を含む大規模言語モデルの学習は、主に2段階で行われます。

#### 事前学習（Pre-training）

- **自己教師あり学習**: 大量のテキストから次の単語を予測するタスクを通じて学習します
- **因果言語モデリング**: GPT系モデルでは、過去のトークンの情報を基に次のトークンを予測する「因果デコーダー」アーキテクチャが採用されています
- **クロスエントロピー損失関数**: 各トークンの予測確率から算出される損失を最小化するように、モデルのパラメーターが更新されます

#### 微調整（Fine-tuning）

- **人間のフィードバックによる強化学習（RLHF）**: 人間のアノテーターから得たフィードバックを基に、モデルの出力を改善します
- **指示データの活用**: GPT-4自体を使って指示に従うデータを生成し、より小規模なモデルを微調整する研究も行われています

### データセット作成の最近の取り組み

最新の研究では、より高品質なデータセットを作成するための工夫も見られます。

- **無料GPT-4アプリを活用したデータ収集**: 公開されたGPT-4アプリを通じて質の高い指示（Instruction）データを収集する試み
- **クリーンデータセットの構築**: 商用可能なモデル作成のために、高品質で使用制限のないデータセットの構築に取り組む事例
- **計算リソースの最適化**: データセットのサイズが大きくなるほど前処理に時間がかかるため、並列処理や処理順序の最適化を行う技術

### 結論

GPT-4を含む大規模言語モデルの学習データ作成は、膨大なデータ収集、入念な前処理とクリーニング、適切なトークン化と変換、そして効果的な学習プロセスという複数の段階を経て行われています。とくにデータの品質は最終的なモデルの性能に大きく影響するため、重複や低品質なコンテンツの削除、有害コンテンツのフィルタリングなどの前処理が非常に重要です。

また、現在でも研究者やエンジニアたちはより効率的なデータ収集方法やクリーニング技術の開発に取り組んでおり、GPU時間やストレージなどのコンピューティングリソースの制約の中で、いかに高品質な学習データを作成するかという課題に挑戦し続けています。
